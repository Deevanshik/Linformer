# Linformer â€“ A Practical Way to Scale Transformers Efficiently

This repository contains an implementation of the Linformer model, a variant of the Transformer architecture that reduces the complexity of self-attention from O(nÂ²) to O(n), making it more efficient for longer sequences.

All code is provided in a single Jupyter Notebook and includes training, inference, and ablation experiments comparing Linformer with standard Transformer models.

---

## ðŸ““ Project Contents

- âœ… Linformer and standard Transformer model implementations
- âœ… Training pipeline and inference code
- âœ… Loss curves and performance visualizations
- âœ… Ablation studies: training & inference time comparison




